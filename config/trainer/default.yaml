# trainer/default
# * [ ] Instantiation of trainers.

# https://github.com/facebookresearch/hydra/issues/2263
# _target_: trainer.trainer.MainTrainer
# _recursive_: False

# == Default List ==
defaults:
    - optimizer: adam
    - lr_skd: cos

# == Core ==
device: "mps:0"
epochs: 1
grad_accum_steps: 1
max_grad_norm: 5
batch_scheduler: True
use_amp: False
# Loss criterion
loss_fn:
    _target_: criterion.custom.MaskedLoss
    name: l1
    # ===
    # Refactor custom ops
    # rescale: True
    # cl:
    #     lv_up_period: 2500
    #     task_lv_max: 12
    # ===
# Dataloader
dataloader:
    batch_size: 64
    shuffle: True
    num_workers: 0
    pin_memory: False
    drop_last: False
# Evaluator
evaluator:
    _target_: evaluating.evaluator.Evaluator
    metric_names:
        - mmae
        - mrmse
        - mmape
    horiz_cuts:
        - 3
        - 6
        - 12

# == Callbacks ==
# Early stopping
es:
    patience: 0
    mode: null
# Model checkpoint
model_ckpt:
    ckpt_metric: null
    ckpt_mode: min
    best_ckpt_mid: last

# == Debug ==
# If True, `_train_stop` stops after one batch, which is useful for
# overfit one batch.
one_batch_only: False
